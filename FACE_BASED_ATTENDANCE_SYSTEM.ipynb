{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e606556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODINGS OF IMAGES SUCCESSFULLY DONE\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n",
      "2\n",
      "20BCE2771-AYUSH GIRI\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import face_recognition\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pyodbc\n",
    "\n",
    "path = 'Image'      #Creating a path for our Images folder\n",
    "Image=[]            #Inserting all the image in an array\n",
    "studentname=[]      #Name of all the students\n",
    "list_of_student=os.listdir(path)  #creating a list of images present in the image path in list of students\n",
    "#print(list_of_student)         \n",
    "#seperating the name of images from our list to names only i.e splitting text and .jpg file : \n",
    "for current_image in list_of_student:\n",
    "    image_read_current=cv2.imread(f'{path}/{current_image}')  #Reading all the images under variable imagereadcurrent\n",
    "    #inserting the current images in the Image[] array declared above using append in the Image variable :\n",
    "    Image.append(image_read_current)\n",
    "    #assigning names to each images by splitting the 0th component(name of student) and the 1st component(extension)\n",
    "    studentname.append(os.path.splitext(current_image)[0])\n",
    "#print(studentname)\n",
    "\n",
    "\n",
    "#GENERATING FACE ENCODINGS FOR N NUMBER OF IMAGES IN THE DATASET TO PREVENT MULTIPLE ENCODINGS  \n",
    "\n",
    "#Creating A function named encodings \n",
    "def encoder(Image): #passing Image as parameter (using encoding the dlib library encodes the image into 128 different features)\n",
    "    encodedlist=[]\n",
    "    for image_access in Image:                                            \n",
    "        encodedlist\n",
    "        img=cv2.cvtColor(image_access,cv2.COLOR_BGR2RGB) #since CV2 reads image in BGR format we need to convert it into RGB\n",
    "        encode=face_recognition.face_encodings(image_access)[0]\n",
    "        encodedlist.append(encode)\n",
    "    return encodedlist\n",
    "#print(encoder(Image))\n",
    "#THIS FACE_ENCODINGS ATTRIBUTE USES HOG ALGORITHM (HOG TRANSFORMATION) INOREDER TO ENCODE THE IMAGES INTO FEATURES\n",
    "encodedvalues=encoder(Image)\n",
    "if (encodedvalues != 0):\n",
    "    print(\"ENCODINGS OF IMAGES SUCCESSFULLY DONE\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "#CAPTURING LIVE VIDEO  USING CV2:\n",
    "CAM=cv2.VideoCapture(0)       #Zero is used for capturing videos live: \n",
    "Status=\"Absent\"\n",
    "while True :\n",
    "    ret,frame=CAM.read()\n",
    "    #The resolution of some camera maybe more for some users and less for others thus we need to resize it into a standard resolution\n",
    "    student_faces=cv2.resize(frame,(0,0),None,0.25,0.25) #Setting Destination to None and decreasing the size by 1/4 in both X and Y \n",
    "    #There Occurs a small problem here that since the video is captured from the camera itself the format is BGR thus we need to convert it into RGB\n",
    "    student_faces=cv2.cvtColor(student_faces,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "#FINDING OUT FACE LOCATIONS OF EACH IMAGES:\n",
    "    face_finder=face_recognition.face_locations(student_faces)#SEARCHES FACES IN THE CURRENT FRAME OF A VIDEO\n",
    "    face_encoder=face_recognition.face_encodings(student_faces,face_finder)#encodes current frame into 128 features\n",
    "    \n",
    "#USING THE FACE_FINDER AND FACE_ENCODER VARIABLES WE MAtCH FACES WITH THE PICTURES:\n",
    "    for encode_the_face , face_location in zip(face_encoder,face_finder):  \n",
    "        face_match=face_recognition.compare_faces(encodedvalues,encode_the_face)#Compare_faces attribute uses two parameters our encoded list which has face features and encode_the_face variable which stores the encoded values from camera \n",
    "#CALCULATING FACE DISTANCES \n",
    "        face_distance=face_recognition.face_distance(encodedvalues,encode_the_face)#Face_distance attribute also uses encoded_values form our training image and encoded values captured in real time\n",
    "        #IF FACE_DISTANCE IS LESS THEN FACES ARE MATCHED IF FACE DISTANCE IS MORE FACES ARE NOT MATCHED:\n",
    "        #CALCULATING DISTANCE BETWEEN TRAINING IMAGE AND REAL TIME FRAME : \n",
    "        matched_reference=np.argmin(face_distance)\n",
    "        #FINDS OUT THE INDEX VALUE OF THE MINIMUM VALUE INSIDE FACE_DISTANCE\n",
    "        \n",
    "        if face_match[matched_reference]:     #TO CHECK IF THE FACE CAPTURED BY CAMERA AND ITS ENCODED VALUE MATCHES WITH THE ENCODED VALUE OF OUR TRAINING IMAGE: \n",
    "            name_of_student=studentname[matched_reference].upper()\n",
    "            print(name_of_student)\n",
    "        else: \n",
    "            print(\"Student not found in the database or please enter the frame:\")\n",
    "            \n",
    "            \n",
    "            #(THIS WILL RECOGNIZE ME :)\n",
    "            y1,x2,y2,x1=face_location\n",
    "            y1,x2,y2,x1=y1*4,x2*4,y2*4,x1*4\n",
    "            cv2.rectangle(frame,(x1,y1),(x2,y2),(0,255,0),3)\n",
    "            cv2.rectangle(frame,(x1,y2-35),(x2,y2),(0,255,0),cv2.FILLED)\n",
    "            cv2.putText(frame,name_of_student,(x1+6,y2-6),cv2.FONT_HERSHEY_COMPLEX,1,(255,255,255),2)\n",
    "            Time=datetime.now().time()\n",
    "            Date=datetime.now().date()\n",
    "           \n",
    "    cv2.imshow('Frames',frame)\n",
    "    if cv2.waitKey(1)& 0xF==ord('q'):\n",
    "        break\n",
    "\n",
    "CAM.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cfaf02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4396d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
